# -*- coding: utf-8 -*-
"""Copy of Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oJIjFn5DqBQzO-brpMttac_jXQhRcm57
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import pickle
import matplotlib.pyplot as plt

import pandas as pd
import numpy as np

#  Load dataset
df = pd.read_csv("/content/superstore_cleaned (1).csv")

#  Identify categorical & numerical columns
categorical_cols = df.select_dtypes(include=['object']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

#  Apply Label Encoding to categorical columns
from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

#  Save Label Encoders for inference
import pickle
with open("label_encoders.pkl", "wb") as f:
    pickle.dump(label_encoders, f)

# Apply Min-Max Scaling to numerical columns
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Save the scaler for future use
with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)

# Split features (X) and target (y)
target_column = "Sales"  # Change this if your target column has a different name
X = df.drop(columns=[target_column])  # Drop the target column from features
y = df[target_column]  # Store only the target column

# Convert to NumPy arrays
X = np.array(X, dtype=np.float32)  # Convert X to a NumPy array
y = np.array(y, dtype=np.float32).reshape(-1, 1)  # Convert y and reshape for PyTorch

import torch

# Convert to PyTorch tensors
X_tensor = torch.tensor(X)
y_tensor = torch.tensor(y)

print(f"X Tensor Shape: {X_tensor.shape}")
print(f"y Tensor Shape: {y_tensor.shape}")

# Define dataset & split into train and test
dataset = TensorDataset(X_tensor, y_tensor)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size

train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# Create DataLoaders
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define the neural network
class SalesPredictionNN(nn.Module):
    def __init__(self, input_size):
        super(SalesPredictionNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

#  Initialize model
input_size = X.shape[1]  # Number of features
model = SalesPredictionNN(input_size)

# Define loss function & optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import torch

#  Assuming X and y are your feature matrix and target variable
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert NumPy arrays to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)

X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)

# Create PyTorch Dataset & DataLoader
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Define optimized model with batch normalization & dropout
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.bn1 = nn.BatchNorm1d(hidden_size)  # Batch Norm Added
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)  # Dropout Increased
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Initialize model
input_size = X_train.shape[1]
hidden_size = 128
output_size = 1  # Regression task
model = NeuralNetwork(input_size, hidden_size, output_size)

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # More frequent LR reduction

# Early stopping
best_val_loss = float("inf")
patience = 5
trigger_times = 0

# Training loop
epochs = 35  # Increase if needed
train_losses = []
val_losses = []

for epoch in range(epochs):
    ### TRAINING PHASE ###
    model.train()
    total_train_loss = 0

    for batch in train_loader:
        X_batch, y_batch = batch
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    ### VALIDATION PHASE  ###
    model.eval()
    total_val_loss = 0

    with torch.no_grad():
        for batch in val_loader:
            X_val, y_val = batch
            val_predictions = model(X_val)
            val_loss = criterion(val_predictions, y_val)
            total_val_loss += val_loss.item()

    avg_val_loss = total_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    scheduler.step()  # Adjust learning rate

    # Early stopping check
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        trigger_times = 0
    else:
        trigger_times += 1
        if trigger_times >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

# Plot Loss Curves
plt.plot(range(len(train_losses)), train_losses, label="Training Loss", color='blue')
plt.plot(range(len(val_losses)), val_losses, label="Validation Loss", color='red')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training & Validation Loss Curve")
plt.legend()
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import torch # Added this import
from sklearn.model_selection import train_test_split


# Set model to evaluation mode
model.eval()  # Assuming 'model' is already defined

# Split into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)  # 30% for validation and test
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split temp into validation and test equally

# Convert test data to PyTorch tensor (if it's not already)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32) # Assuming 'X_test' is already defined

# Forward pass to get predictions
with torch.no_grad():  # Disable gradient computation
    y_pred = model(X_test_tensor)  # Defining y_pred here

# Apply activation function (Sigmoid for binary classification)
# y_pred = torch.sigmoid(y_pred)  # Use softmax for multi-class - Commenting out as not needed for regression
# Convert probabilities to class labels
# y_pred_classes = (y_pred > 0.5).int()  # For binary classification - Commenting out as not needed for regression

# Convert predictions to NumPy array if it's a tensor
y_pred_np = y_pred.detach().numpy() if hasattr(y_pred, 'detach') else y_pred
y_test_np = y_test  # Keep as-is since it's already a NumPy array

# Compute Regression Metrics
mae = mean_absolute_error(y_test_np, y_pred_np)
mse = mean_squared_error(y_test_np, y_pred_np)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_np, y_pred_np)

# Print results
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"RÂ² Score: {r2:.4f}")

import torch
import os # Import the os module

# Define the path to save the model
model_path = "model_training/model_weights.pth"

# Create the directory if it doesn't exist
os.makedirs(os.path.dirname(model_path), exist_ok=True)

# Save the trained model
torch.save(model.state_dict(), model_path)

print(f"Model saved successfully at {model_path}")